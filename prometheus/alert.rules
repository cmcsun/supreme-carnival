groups:
- name: containers
  rules:

  # Alert for any instance that is unreachable for >2 minutes.
  - alert: service_down
    expr: up == 0
    for: 119s
    labels:
      severity: critical
    annotations:
      summary: "Instance {{ $labels.instance }} down"
      description: "{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 2 minutes."

  - alert: high_cpu_load
    expr: node_load1 > 0.5
    for: 3m
    labels:
      severity: warning
    annotations:
      summary: "Instance {{ $labels.instance }} under high load"
      description: "{{ $labels.instance }} of job {{ $labels.job }} is under high CPU load."

- name: host
  rules:
  - alert: high_cpu_load
    expr: node_load1 > 1.5
    for: 30s
    labels:
      severity: warning
    annotations:
      summary: "Server under high load"
      description: "Docker host is under high load, the avg load 1m is at {{ $value}}. Reported by instance {{ $labels.instance }} of job {{ $labels.job }}."

  - alert: high_memory_load
    expr: (sum(node_memory_MemTotal_bytes) - sum(node_memory_MemFree_bytes + node_memory_Buffers_bytes + node_memory_Cached_bytes) ) / sum(node_memory_MemTotal_bytes) * 100 > 85
    for: 30s
    labels:
      severity: warning
    annotations:
      summary: "Server memory is almost full"
      description: "Docker host memory usage is {{ humanize $value}}%. Reported by instance {{ $labels.instance }} of job {{ $labels.job }}."

  - alert: high_storage_load
    expr: (node_filesystem_size_bytes{fstype="aufs"} - node_filesystem_free_bytes{fstype="aufs"}) / node_filesystem_size_bytes{fstype="aufs"}  * 100 > 85
    for: 30s
    labels:
      severity: warning
    annotations:
      summary: "Server storage is almost full"
      description: "Docker host storage usage is {{ humanize $value}}%. Reported by instance {{ $labels.instance }} of job {{ $labels.job }}."

#- name: containers
#  rules:
#  - alert: jenkins_down
#    expr: absent(container_memory_usage_bytes{name="jenkins"})
#    for: 30s
#    labels:
#      severity: critical
#    annotations:
#      summary: "Jenkins down"


#- name: TraefikAlerts
#  rules:
#    - alert: TraefikServiceDown
#      expr: 'count(traefik_service_server_up) by (service) == 0'
#      for: 0m
#      labels:
#        severity: critical
#        customer: true
#      annotations:
#        summary: Traefik service down (instance {{ $labels.instance }})
#        description: "All Traefik services are down\n  VALUE = {{ $value }}\n  LABELS = {{ $labels
# }}"
#
#    - alert: TraefikHighHttp4xxErrorRateService
#      expr: 'sum(rate(traefik_service_requests_total{code=~"4.*"}[3m])) by (service) / sum(rate(tr
#aefik_service_requests_total[3m])) by (service) * 100 > 5'
#      for: 1m
#      labels:
#        severity: critical
#      annotations:
#        summary: Traefik high HTTP 4xx error rate service (instance {{ $labels.instance }})
#        description: "Traefik service 4xx error rate is above 5%\n  VALUE = {{ $value }}\n  LABELS
# = {{ $labels }}"
#
#    - alert: TraefikHighHttp5xxErrorRateService
#      expr: 'sum(rate(traefik_service_requests_total{code=~"5.*"}[3m])) by (service) / sum(rate(tr
#aefik_service_requests_total[3m])) by (service) * 100 > 5'
#      for: 1m
#      labels:
#        severity: critical
#      annotations:
#        summary: Traefik high HTTP 5xx error rate service (instance {{ $labels.instance }})
#        description: "Traefik service 5xx error rate is above 5%\n  VALUE = {{ $value }}\n  LABELS
# = {{ $labels }}"
#
##- name: PrometheusSelfMonitoring
##  rules:
##    - alert: PrometheusJobMissing
##      expr: 'absent(up{job="prometheus"})'
#      for: 0m
#      labels:
#        severity: warning
#      annotations:
#        summary: Prometheus job missing (instance {{ $labels.instance }})
#        description: "A Prometheus job has disappeared\n  VALUE = {{ $value }}\n  LABELS = {{ $lab
#els }}"
#
#    - alert: ZCastSystemComponentNotReporting
#      expr: 'up{job!="players"} == 0'
#      for: 0m
#      labels:
#        severity: critical
#        customer: true
#      annotations:
#        summary: Prometheus target missing (instance {{ $labels.instance }})
#        description: "A ZCAST System component has stopped reporting promethus metrics. The node e
#xporter might be crashed."
#
#    - alert: ZCastPlayerNotReporting
#      expr: 'up{job="players"} == 0'
#      for: 0m
#      labels:
#        severity: critical
#        customer: true
#      annotations:
#        summary: Prometheus target missing (instance {{ $labels.instance }})
#        description: "A player has stopped reporting promethus metrics. The node exporter might be
# crashed."
#
#    - alert: PrometheusAllTargetsMissing
#      expr: 'sum by (job) (up) == 0'
#      for: 0m
#      labels:
#        severity: critical
#      annotations:
#        summary: Prometheus all targets missing (instance {{ $labels.instance }})
#        description: "A Prometheus job does not have living target anymore.\n  VALUE = {{ $value }
#}\n  LABELS = {{ $labels }}"
#
